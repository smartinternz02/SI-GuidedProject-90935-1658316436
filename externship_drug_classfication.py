# -*- coding: utf-8 -*-
"""Externship Drug Classfication

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DTm-T6ZmO9DxCJMO2I_7lfcNYQcm4Pix
"""

#importing the necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn import model_selection
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import RandomForestClassifier
import warnings
warnings.filterwarnings('ignore')

data = pd.read_csv(r"/content/drug200.csv")              #reading the dataset
data

type(data)

data.head()           #gives 1st 5 rows of the data or we can mention any particular value

data.tail()  #gives last 5 rows of the data or we can mention any particular value

data.info()                       #gives info abt your data

"""## **Descriptive Analysis**"""

data.describe(include='all')     #gives the stastical info about data set

data.isnull().any()                      #to check there is any null value in our data

"""## Univariate Analysis"""

# Checking the distribution (normal or skewed)

plt.figure(figsize=(12,5))
plt.subplot(121)
sns.distplot(data['Age'],color='r')
plt.subplot(122)
sns.distplot(data['Na_to_K'])
plt.show()

# From the above plot age column is normally distributed. Na_to_k is right skewed (mean>mode). To overcome skewness transformation techniques can be used.

print(stats.mode(data['Na_to_K']))
print(np.mean(data['Na_to_K']))

# Finding outliers

plt.figure(figsize=(12,5))
plt.subplot(121)
sns.boxplot(data['Age'])
plt.subplot(122)
sns.boxplot(data['Na_to_K'])
plt.show()

# Na_to_K has 8 outliers. In this project we are not going to handle outliers. Most of the classification algorithms are not sensitive to outliers.

q1 = np.quantile(data['Na_to_K'],0.25)
q3 = np.quantile(data['Na_to_K'],0.75)

IQR = q3-q1

upper_bound = q3+(1.5*IQR)

print('Upper Bound :',upper_bound)

print('Skewed data :',len(data[data['Na_to_K']>upper_bound]))

# Creating a data frame with categorical features for following visualization

data_k= data.select_dtypes(include='object')
data_k.head()

# Visualizing the count of categorical variable.

plt.figure(figsize=(18,4))
for i,j in enumerate(data_k):
    plt.subplot(1,4,i+1)
    sns.countplot(data_k[j])

"""## **Bivariate Analysis**"""

data.head()

# Visualizing the relation between drug, BP, sex & cholesterol

plt.figure(figsize=(20,5))
plt.subplot(131)
sns.countplot(data['Drug'],hue=data['BP'])
plt.legend(loc='upper right')
plt.subplot(132)
sns.countplot(data['Drug'],hue=data['Sex'])
plt.subplot(133)
sns.countplot(data['Drug'],hue=data['Cholesterol'])

data.head()

"""## **Multivariate Analysis**"""

sns.swarmplot(data['Drug'],data['Na_to_K'],hue=data['BP'])

# DrugC is used for low BP patient, DrugY is used on patients having Na_to_K > 15

"""### Encoding- Converting categorical to **numerical**"""

#system cannot understand categorical/textual data so we convert them to numerical data
#converting textual data to numerical data
#This is my possible by Label encoder

data['Sex'].unique()

data['BP'].unique()

data['Cholesterol'].unique()

data['Drug'].unique()

"""we use label encoder to convert textual data to numerical data"""

#we use label encoder to convert textual data to numerical data
#we have create an instance to labelencoder
le= LabelEncoder()
data['Sex']= le.fit_transform(data['Sex'])
data['BP']= le.fit_transform(data['BP'])
data['Cholesterol']= le.fit_transform(data['Cholesterol'])

data.head()

data.tail()

data['Sex'].unique()

data['BP'].unique()

data['Cholesterol'].unique()

data['Drug'].unique()

"""Converting Numerically converted data to Binary Data
 
 onehotencoder converts numerically converted data to categorical data

"""

data.info()

data.head()

data['Drug'].value_counts()

x = data.drop('Drug',axis=1)
x.head()

y = data['Drug']
y.head()

#split our data into dependent and independent columns
"""The iloc() function in python is defined in the Pandas module that helps us to select a specific row or column from the data set.

  >>Using the iloc method in python, we can easily retrieve any particular value from a row or column by using index values.
 The iloc function in python takes two optional parameters i.e. row number(s) and column number(s).
  We can only pass integer type values as parameter(s) in the iloc function in python.
x=data.iloc[:,0:5].values
y=data.iloc[:,5].values """

x.shape

y.shape

#split data into train and test
x_train,x_test,y_train,y_test= train_test_split(x,y,test_size=0.2, random_state=0)

print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

x_train

x_test

y_train

y_test

#feature scaling
#standard scaling= value-mean/standard deviation
sc= StandardScaler()
x_train= sc.fit_transform(x_train)
x_test= sc.fit_transform(x_test)

# default regression
from sklearn.metrics import classification_report,accuracy_score,plot_confusion_matrix
rf=RandomForestClassifier()
rf.fit(x_train,y_train)
y_pred_model =rf.predict(x_test)
print('accuracy_score = ',accuracy_score(y_test, y_pred_model))
print(classification_report(y_test, y_pred_model))
plot_confusion_matrix(rf, x_test, y_test)
plt.show()

# default regression
decision_tree_classifier_model = DecisionTreeClassifier()
kfold = model_selection.KFold(n_splits=10)
cv = model_selection.cross_val_score(decision_tree_classifier_model, x_train, y_train, cv=kfold)
cv_mean = cv.mean()
cv_std = cv.std()
print('mean =',cv_mean)
print('std =',cv_std)
decision_tree_classifier_model.fit(x_train, y_train)
y_pred_model =decision_tree_classifier_model.predict(x_test)
print('accuracy_score = ',accuracy_score(y_test, y_pred_model))
print(classification_report(y_test, y_pred_model))
plot_confusion_matrix(decision_tree_classifier_model, x_test, y_test)
plt.show()

import pickle
from scipy import stats
pickle.dump(rf,open('model.pkl','wb'))

